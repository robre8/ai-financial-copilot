# ðŸ¢ Enterprise Guide - AI Financial Copilot

> For production deployments, compliance requirements, and advanced configurations. See [README.md](./README.md) for quick start.

## ðŸ“‘ Table of Contents

1. [Enterprise Architecture](#enterprise-architecture)
2. [Security & Compliance](#security--compliance)
3. [Authentication & Authorization](#authentication--authorization)
4. [Deployment & Infrastructure](#deployment--infrastructure)
5. [Monitoring & Observability](#monitoring--observability)
6. [Scalability & Performance](#scalability--performance)
7. [Disaster Recovery](#disaster-recovery)
8. [Operational Runbooks](#operational-runbooks)
9. [Integration Patterns](#integration-patterns)
10. [Cost Optimization](#cost-optimization)

---

## Enterprise Architecture

### High-Level Deployment Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Client Applications                      â”‚
â”‚ (Internal Dashboard, Mobile App, Third-party Integrations) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTPS
                     â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      API Gateway / Load Balancer       â”‚
    â”‚      (Kong, Nginx, AWS ALB)            â”‚
    â”‚      - Rate limiting                   â”‚
    â”‚      - Request routing                 â”‚
    â”‚      - SSL/TLS termination             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“            â†“            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ FastAPI â”‚ â”‚ FastAPI â”‚ â”‚ FastAPI â”‚  (Kubernetes Pods)
    â”‚ Replica â”‚ â”‚ Replica â”‚ â”‚ Replica â”‚  - Horizontal autoscaling
    â”‚    1    â”‚ â”‚    2    â”‚ â”‚    3    â”‚  - Health checks
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚           â”‚           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“            â†“             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Redis   â”‚ â”‚ Postgresâ”‚ â”‚  S3/GCS  â”‚
    â”‚ Cache   â”‚ â”‚pgvector â”‚ â”‚ Blob     â”‚
    â”‚         â”‚ â”‚(Vector  â”‚ â”‚ Storage  â”‚
    â”‚         â”‚ â”‚ DB)     â”‚ â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â†“             â†“
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Groq API â”‚  â”‚HuggingFace
                 â”‚  (LLM)   â”‚  â”‚(Embeddings)
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Microservices Breakdown

| Service | Responsibility | Tech |
|---------|------------------|------|
| **API Gateway** | Route, auth, rate limit, SSL | Kong / AWS ALB |
| **FastAPI Replicas** | Core logic, request handling | FastAPI + Uvicorn |
| **Vector DB** | Semantic search, embeddings | PostgreSQL + pgvector |
| **Cache Layer** | Query caching, session storage | Redis |
| **Document Storage** | PDF archival and versioning | AWS S3 / Google Cloud Storage |
| **Message Queue** | Async document processing | RabbitMQ / AWS SQS |
| **Monitoring** | Logs, metrics, tracing | Prometheus + Grafana + Jaeger |
| **Auth Service** | JWT validation, RBAC | Keycloak / AWS Cognito |

---

## Security & Compliance

### 1. Authentication & Identity

#### JWT-Based Authentication
```python
# backend/app/core/security.py
from fastapi_jwt_extended import JWTManager, create_access_token
from fastapi.security import HTTPBearer, HTTPAuthCredentialDetails

jwt_manager = JWTManager()
security = HTTPBearer()

# Protected endpoint example
@router.post("/ask")
async def ask(
    question: str,
    credentials: HTTPAuthCredentialDetails = Depends(security)
):
    token = credentials.credentials
    claims = jwt_manager.decode_token(token)
    user_id = claims.get("sub")
    # Process request for authenticated user
    ...
```

#### Environment Setup
```bash
# backend/.env
JWT_ALGORITHM=HS256
JWT_SECRET_KEY=your-super-secret-key-min-32-chars
JWT_EXPIRATION_HOURS=24
OAUTH2_PROVIDERS=google,azure,okta  # Multi-tenant support
```

### 2. Data Protection

#### Encryption at Rest
```python
# Use AWS Secrets Manager or HashiCorp Vault for sensitive data
import boto3

secrets_client = boto3.client('secretsmanager')

def get_api_key(key_name: str):
    response = secrets_client.get_secret_value(SecretId=key_name)
    return response['SecretString']
```

#### Encryption in Transit
- **All traffic**: HTTPS/TLS 1.3 minimum
- **Certificate**: Automated renewal with Let's Encrypt or AWS ACM
- **HSTS**: Enabled for browsers (HTTP Strict-Transport-Security)

#### Field-Level Encryption
```python
# For sensitive document metadata
from cryptography.fernet import Fernet

cipher_suite = Fernet(your_key)

encrypted_data = cipher_suite.encrypt(b"sensitive_info")
decrypted_data = cipher_suite.decrypt(encrypted_data)
```

### 3. Compliance Requirements

#### GDPR (EU Users)
- âœ… **Right to erasure** (delete user data)
- âœ… **Data portability** (export user data)
- âœ… **Privacy by design** (minimal data collection)
- âœ… **DPA** (Data Processing Agreement with vendors)

```python
@router.delete("/users/{user_id}/data")
async def gdpr_right_to_erasure(user_id: str):
    """Delete all user data including vector embeddings"""
    db.delete_user_documents(user_id)
    db.delete_user_embeddings(user_id)
    db.delete_user_account(user_id)
    log_audit_event("GDPR_ERASURE", user_id)
    return {"status": "success"}
```

#### SOC 2 Type II
- âœ… **Availability**: 99.95% uptime SLA
- âœ… **Security**: Encryption, access controls, vulnerability scanning
- âœ… **Integrity**: Checksums, audit logs
- âœ… **Confidentiality**: RBAC, encryption
- âœ… **Privacy**: Data minimization, retention policies

#### PII Handling
- Document PDFs may contain PII (names, emails, account numbers)
- Tokenize or mask PII before vector encoding
- Implement document-level access control

```python
# Mask PII before embedding
import re

def mask_pii(text: str) -> str:
    text = re.sub(r'\b\d{3}-\d{2}-\d{4}\b', '[SSN]', text)  # SSN
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
    return text
```

---

## Authentication & Authorization

### Role-Based Access Control (RBAC)

```python
# backend/app/core/permissions.py
from enum import Enum
from typing import List

class Role(str, Enum):
    ADMIN = "admin"
    ANALYST = "analyst"
    VIEWER = "viewer"

class Permission(str, Enum):
    UPLOAD_DOCUMENT = "upload:document"
    QUERY_DOCUMENT = "query:document"
    DELETE_DOCUMENT = "delete:document"
    EXPORT_REPORT = "export:report"
    MANAGE_USERS = "manage:users"

ROLE_PERMISSIONS = {
    Role.ADMIN: [
        Permission.UPLOAD_DOCUMENT,
        Permission.QUERY_DOCUMENT,
        Permission.DELETE_DOCUMENT,
        Permission.EXPORT_REPORT,
        Permission.MANAGE_USERS,
    ],
    Role.ANALYST: [
        Permission.UPLOAD_DOCUMENT,
        Permission.QUERY_DOCUMENT,
        Permission.EXPORT_REPORT,
    ],
    Role.VIEWER: [
        Permission.QUERY_DOCUMENT,
    ],
}

def require_permission(permission: Permission):
    async def permission_checker(credentials = Depends(security)):
        user = get_user_from_token(credentials.credentials)
        role = user.get("role")
        if permission not in ROLE_PERMISSIONS.get(role, []):
            raise HTTPException(403, "Insufficient permissions")
        return user
    return permission_checker

@router.post("/upload-pdf")
async def upload_pdf(file: UploadFile, user=Depends(require_permission(Permission.UPLOAD_DOCUMENT))):
    # Only ADMIN and ANALYST can upload
    ...
```

### Multi-Tenant Isolation

```python
# backend/app/models.py
from sqlalchemy import Column, String, ForeignKey

class Organization(Base):
    __tablename__ = "organizations"
    id = Column(String, primary_key=True)
    name = Column(String)
    api_key = Column(String, unique=True)

class Document(Base):
    __tablename__ = "documents"
    id = Column(String, primary_key=True)
    organization_id = Column(String, ForeignKey("organizations.id"))  # Tenant isolation
    filename = Column(String)
    created_at = Column(DateTime)

class ReadOnlyQuery(Base):
    __tablename__ = "read_only_queries"
    
    def get_documents_for_org(org_id: str):
        """All queries must filter by org_id"""
        return db.query(Document).filter(Document.organization_id == org_id).all()
```

### API Key Management

```python
# backend/app/core/api_keys.py
import secrets
from datetime import datetime, timedelta

def generate_api_key(organization_id: str) -> str:
    token = secrets.token_urlsafe(32)
    hashed = hash_token(token)  # Store hash, not plain token
    db.save_api_key(organization_id, hashed, expires_at=datetime.now() + timedelta(days=365))
    return token  # Return once to user

def validate_api_key(token: str) -> dict:
    hashed = hash_token(token)
    api_key = db.get_api_key(hashed)
    if not api_key or api_key.expires_at < datetime.now():
        raise HTTPException(401, "Invalid or expired API key")
    return {"org_id": api_key.organization_id}

@router.post("/ask")
async def ask(request: QuestionRequest, api_key: str = Header(...)):
    org = validate_api_key(api_key)
    # Enforce org isolation in all queries
    ...
```

---

## Deployment & Infrastructure

### Kubernetes Deployment

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-financial-copilot
spec:
  replicas: 3  # Horizontal scaling
  selector:
    matchLabels:
      app: copilot
  template:
    metadata:
      labels:
        app: copilot
    spec:
      containers:
      - name: fastapi
        image: ai-financial-copilot:latest
        ports:
        - containerPort: 8000
        env:
        - name: GROQ_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: groq-api-key
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secrets
              key: postgres-url
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
```

### Infrastructure as Code (Terraform)

```hcl
# terraform/main.tf
provider "aws" {
  region = "us-east-1"
}

# PostgreSQL RDS with pgvector extension
resource "aws_db_instance" "postgres" {
  identifier            = "financial-copilot-db"
  engine                = "postgres"
  engine_version        = "15.2"
  instance_class        = "db.t3.medium"
  allocated_storage      = 100
  storage_type          = "gp3"
  storage_encrypted     = true
  multi_az              = true  # Disaster recovery
  backup_retention_days = 30
  
  # Enable pgvector extension
  parameter_group_name = aws_db_parameter_group.postgres.id
}

# Redis for caching
resource "aws_elasticache_cluster" "redis" {
  cluster_id           = "copilot-cache"
  engine               = "redis"
  engine_version       = "7.0"
  node_type            = "cache.t3.medium"
  num_cache_nodes      = 1
  parameter_group_name = "default.redis7"
  engine_version_parameter_group_name = "default.redis7"
  at_rest_encryption_enabled = true
  transit_encryption_enabled = true
}

# S3 for document storage
resource "aws_s3_bucket" "documents" {
  bucket = "financial-copilot-docs"
}

resource "aws_s3_bucket_versioning" "documents" {
  bucket = aws_s3_bucket.documents.id
  versioning_configuration {
    status = "Enabled"  # Document versioning
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "documents" {
  bucket = aws_s3_bucket.documents.id
  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

# ECS Cluster for FastAPI
resource "aws_ecs_cluster" "copilot" {
  name = "financial-copilot"
}

resource "aws_ecs_service" "copilot" {
  name            = "copilot"
  cluster         = aws_ecs_cluster.copilot.id
  task_definition = aws_ecs_task_definition.copilot.arn
  desired_count   = 3
  launch_type     = "FARGATE"
  
  load_balancer {
    target_group_arn = aws_lb_target_group.copilot.arn
    container_name   = "fastapi"
    container_port   = 8000
  }
}
```

---

## Monitoring & Observability

### Structured Logging (ELK Stack)

```python
# backend/app/core/logger.py
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_obj = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        if record.exc_info:
            log_obj["exception"] = self.formatException(record.exc_info)
        return json.dumps(log_obj)

def setup_logger():
    logger = logging.getLogger("copilot")
    handler = logging.StreamHandler()
    handler.setFormatter(JSONFormatter())
    logger.addHandler(handler)
    return logger

logger = setup_logger()

# Usage in code
logger.info("pdf_uploaded", extra={
    "user_id": user_id,
    "document_id": doc_id,
    "file_size": file_size,
})
```

### Metrics & Tracing (Prometheus)

```python
# backend/app/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# Counters
uploads_total = Counter(
    'document_uploads_total',
    'Total documents uploaded',
    ['organization_id', 'status']
)

# Histograms (response times)
query_duration_seconds = Histogram(
    'query_duration_seconds',
    'Query response time',
    ['model', 'status'],
    buckets=(0.5, 1, 2, 5, 10)
)

# Gauges (current state)
active_embeddings = Gauge(
    'active_embeddings',
    'Number of vectors in index'
)

# Usage
@router.post("/upload-pdf")
async def upload_pdf(file: UploadFile):
    try:
        # ... upload logic
        uploads_total.labels(organization_id=org_id, status="success").inc()
    except Exception as e:
        uploads_total.labels(organization_id=org_id, status="failure").inc()
        raise

@router.post("/ask")
async def ask(request: QuestionRequest):
    with query_duration_seconds.labels(model="llama-3.1-8b", status="success").time():
        # ... query logic
        pass
```

### Distributed Tracing (Jaeger)

```python
# backend/app/core/tracing.py
from opentelemetry import trace, metrics
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

jaeger_exporter = JaegerExporter(
    agent_host_name="jaeger-agent",
    agent_port=6831,
)
trace.set_tracer_provider(TracerProvider())
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(jaeger_exporter)
)

FastAPIInstrumentor().instrument_app(app)

# Manual tracing in services
tracer = trace.get_tracer(__name__)

@staticmethod
def ask(query: str) -> dict:
    with tracer.start_as_current_span("rag_pipeline") as span:
        span.set_attribute("query", query)
        
        with tracer.start_as_current_span("embedding") as span:
            q_emb = EmbeddingService.embed_text(query)
        
        with tracer.start_as_current_span("vector_search") as span:
            context_chunks = RAGService.vector_store.search(q_emb, k=3)
        
        with tracer.start_as_current_span("llm_generation") as span:
            answer = LLMService.generate(context_chunks, query)
        
        return {"answer": answer, ...}
```

### Health Checks & Alerts

```python
# backend/app/api/health.py
from fastapi import APIRouter

router = APIRouter()

@router.get("/health")
async def health():
    """Liveness probe for Kubernetes"""
    return {"status": "ok"}

@router.get("/ready")
async def readiness():
    """Readiness probe - checks all dependencies"""
    checks = {
        "database": await check_database(),
        "redis": await check_redis(),
        "groq_api": await check_groq(),
        "huggingface": await check_embeddings(),
    }
    
    if not all(checks.values()):
        raise HTTPException(503, "Service not ready", detail=checks)
    
    return {"status": "ready"}

async def check_database():
    try:
        await db.execute("SELECT 1")
        return True
    except:
        return False
```

---

## Scalability & Performance

### Horizontal Scaling Strategy

| Component | Scale Method | Limits |
|-----------|------------|--------|
| **FastAPI App** | K8s HPA (CPU >75%) | 50 replicas |
| **PostgreSQL** | Read replicas + sharding | 10TB+ with partitioning |
| **Redis** | Redis Cluster | 16 nodes max |
| **Groq API** | Batch requests | Rate: 500 req/min (free tier) |
| **S3 Storage** | Unlimited | Pay per GB |

### Database Optimization

```sql
-- Create pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Optimized embeddings table
CREATE TABLE embeddings (
    id UUID PRIMARY KEY,
    organization_id UUID NOT NULL,
    document_id UUID NOT NULL,
    chunk_text TEXT NOT NULL,
    embedding vector(384),  -- 384-dimensional
    created_at TIMESTAMP DEFAULT NOW(),
    
    FOREIGN KEY (organization_id) REFERENCES organizations(id) ON DELETE CASCADE,
    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE
);

-- Create IVFFlat index for fast similarity search
CREATE INDEX ON embeddings 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);

-- Partition by organization for multi-tenancy
CREATE TABLE embeddings_org1 PARTITION OF embeddings
    FOR VALUES IN ('org-uuid-1');

-- Query with reranking (Reciprocal Rank Fusion)
SELECT id, chunk_text, embedding <-> $1::vector as distance
FROM embeddings
WHERE organization_id = $2::UUID
ORDER BY distance
LIMIT 10;
```

### Query Caching Strategy

```python
# backend/app/core/cache.py
import redis
import json
import hashlib
from datetime import timedelta

redis_client = redis.Redis(host="localhost", port=6379, db=0)

def get_cache_key(user_id: str, question: str) -> str:
    question_hash = hashlib.md5(question.encode()).hexdigest()
    return f"answer:{user_id}:{question_hash}"

def cache_answer(user_id: str, question: str, answer: dict, ttl_hours: int = 24):
    key = get_cache_key(user_id, question)
    redis_client.setex(
        key,
        timedelta(hours=ttl_hours),
        json.dumps(answer)
    )

def get_cached_answer(user_id: str, question: str) -> dict | None:
    key = get_cache_key(user_id, question)
    cached = redis_client.get(key)
    return json.loads(cached) if cached else None

@router.post("/ask")
async def ask(request: QuestionRequest, user = Depends(get_current_user)):
    # Try cache first
    cached = get_cached_answer(user["id"], request.question)
    if cached:
        return cached
    
    # Compute if not cached
    result = rag_pipeline.ask(request.question)
    
    # Cache result
    cache_answer(user["id"], request.question, result)
    
    return result
```

---

## Disaster Recovery

### Backup Strategy

| Data | Backup Method | Frequency | Retention |
|------|---------------|-----------|-----------|
| PostgreSQL | AWS RDS automated backups | Daily | 30 days |
| S3 Documents | Versioning + Cross-region replication | Continuous | 7 years |
| Redis | RDB snapshots | Hourly | 7 days |
| Application Code | GitHub (git history) | Per commit | Unlimited |

```hcl
# Terraform: Enable multi-region replication
resource "aws_s3_bucket_replication_configuration" "documents" {
  bucket   = aws_s3_bucket.documents.id
  role     = aws_iam_role.replication.arn
  
  rule {
    status = "Enabled"
    filter {
      prefix = ""
    }
    destination {
      bucket       = aws_s3_bucket.documents_backup.arn
      region       = "eu-west-1"  # Disaster recovery region
      storage_class = "GLACIER"   # Cost-optimized
    }
  }
}
```

### RTO and RPO Targets

- **RTO (Recovery Time Objective)**: 4 hours
- **RPO (Recovery Point Objective)**: 1 hour
- **Failover**: Automated DNS failover to backup region
- **Testing**: Monthly disaster recovery drills

### Runbook: Database Recovery

```bash
#!/bin/bash
# scripts/recovery/postgres_restore.sh

set -e

BACKUP_ID=${1:-latest}
RESTORE_DB="financial_copilot_restored"

echo "Starting PostgreSQL recovery from backup: $BACKUP_ID"

# 1. Get backup snapshot
SNAPSHOT=$(aws rds describe-db-snapshots \
  --db-instance-identifier financial-copilot-db \
  --query "DBSnapshots[0].DBSnapshotIdentifier" \
  --output text)

echo "Using snapshot: $SNAPSHOT"

# 2. Restore from snapshot
aws rds restore-db-instance-from-db-snapshot \
  --db-instance-identifier $RESTORE_DB \
  --db-snapshot-identifier $SNAPSHOT

# 3. Wait for restoration
aws rds wait db-instance-available \
  --db-instance-identifier $RESTORE_DB

echo "Database restored successfully"

# 4. Run integrity checks
psql postgresql://user:pass@$RESTORE_DB.xxxx.us-east-1.rds.amazonaws.com/copilot \
  --command "SELECT COUNT(*) FROM embeddings;"

echo "Recovery complete. Update DNS to point to $RESTORE_DB"
```

---

## Operational Runbooks

### Scaling Operations

#### Emergency Scale-Up
```bash
# 1. Increase replicas immediately
kubectl scale deployment copilot --replicas=10

# 2. Monitor metrics
kubectl top pods -l app=copilot

# 3. Fine-tune resources
kubectl set resources deployment copilot \
  --limits=cpu=1000m,memory=2Gi \
  --requests=cpu=500m,memory=512Mi
```

#### Database Connection Pooling Alert
```python
# Alert: If active connections > 80
if active_connections > db.max_connections * 0.8:
    # 1. Notify team
    send_alert("High database connections: " + active_connections)
    
    # 2. Kill idle connections
    for conn in db.get_idle_connections(idle_for_seconds=300):
        conn.terminate()
    
    # 3. Scale replicas for read
    increase_read_replicas()
```

### Incident Response

#### P1: API Down (99% requests failing)
```
1. Declare incident in Slack #incident-channel
2. Run: kubectl get pods -l app=copilot
3. If OOMKilled: Increase memory limits
4. If CrashLoopBackOff: Check logs with: kubectl logs <pod>
5. Rollback to last known good version: git revert <hash>
6. Update #incidents channel with status every 15 mins
```

#### P2: High Latency (>10 sec response time)
```
1. Check load balancer health: curl /health
2. Check database query performance: EXPLAIN ANALYZE
3. Check Redis hit rate: redis-cli INFO stats
4. Scale horizontally if CPU >80%
5. Investigate slow queries in application logs
```

---

## Integration Patterns

### Enterprise Systems Integration

#### Salesforce Integration
```python
# backend/app/integrations/salesforce.py
from simple_salesforce import Salesforce

sf = Salesforce(
    username=os.getenv("SALESFORCE_USERNAME"),
    password=os.getenv("SALESFORCE_PASSWORD"),
    security_token=os.getenv("SALESFORCE_SECURITY_TOKEN"),
)

@router.post("/integrations/salesforce/analyze")
async def analyze_salesforce_document(account_id: str):
    # 1. Get opportunity from Salesforce
    opportunity = sf.query(f"
        SELECT Id, Name, Description 
        FROM Opportunity 
        WHERE AccountId = '{account_id}' 
        LIMIT 1
    ")
    
    # 2. Generate AI insights using our copilot
    insights = await rag_service.ask(opportunity_description)
    
    # 3. Store insights back in Salesforce
    sf.Opportunity.update(opportunity_id, {
        'AI_Analysis__c': insights['answer'],
        'AI_Model_Used__c': insights['model'],
    })
    
    return insights
```

#### LMS Systems Integration
```python
# backend/app/integrations/lms.py
@router.post("/integrations/lms/upload")
async def upload_from_lms(
    course_id: str,
    document_id: str,
    lms_token: str = Header(...)
):
    # 1. Validate LMS token
    lms_user = validate_lms_token(lms_token)
    
    # 2. Fetch document from LMS
    doc_url = f"https://lms.example.com/api/documents/{document_id}"
    response = requests.get(doc_url, headers={"Authorization": f"Bearer {lms_token}"})
    
    # 3. Upload to our system
    result = await upload_and_index(response.content)
    
    # 4. Notify LMS of completion
    requests.post(f"{doc_url}/processed", json={"status": "success"})
    
    return result
```

### API Marketplace Listing

```json
{
  "api": {
    "title": "AI Financial Copilot API",
    "version": "1.0.0",
    "base_url": "https://api.ai-financial-copilot.com",
    "auth": {
      "type": "apiKey",
      "header": "X-API-Key"
    },
    "endpoints": [
      {
        "path": "/documents/upload",
        "method": "POST",
        "rate_limit": "100/minute",
        "pricing": "free ($0-$2/1000 documents)"
      },
      {
        "path": "/documents/{id}/query",
        "method": "POST",
        "rate_limit": "1000/minute",
        "pricing": "pay-as-you-go ($0.01/query)"
      }
    ]
  }
}
```

---

## Cost Optimization

### Cost Breakdown (Monthly)

| Component | Cost | Optimization |
|-----------|------|--------------|
| AWS ECS (3 replicas, t3.medium) | $120 | Use spot instances (-70%) |
| RDS PostgreSQL (db.t3.medium) | $80 | Reserve instance (-40%) |
| S3 Storage (1TB) | $25 | S3 Glacier for older docs |
| Redis (cache.t3.medium) | $50 | ElastiCache reserved (-35%) |
| **Groq API calls** | $200 | Batch requests, caching |
| **Huggingface API** | $50 | Local model hosting |
| **Total** | **~$525** | â†’ **~$250 optimized** |

### Reserved Instances (45-55% savings)

```hcl
# Terraform: Purchase all 1-year reserved instances
resource "aws_ec2_reserved_instances" "fastapi" {
  instance_class   = "t3.medium"
  count            = 3
  instance_type    = "on-demand"
  offering_class   = "all-upfront"
  term             = "one_year"  # Cheapest option
}
```

### Smart Caching to Reduce API Calls

```python
# Cache strategies reduce external API costs
CACHE_STRATEGIES = {
    "hot": {"ttl": 24*3600, "hit_rate": 0.85},  # Frequently accessed
    "warm": {"ttl": 7*24*3600, "hit_rate": 0.50},  # Less frequent
    "cold": {"ttl": 30*24*3600, "hit_rate": 0.10},  # Rarely accessed
}

# Expected monthly savings:
# 10,000 queries/month
# Cache hit rate: 75% average
# Result: 7,500 cache hits, only 2,500 API calls
# Savings: 7,500 * $0.01 = $75/month
```

---

## SLA & Support

### Service Level Agreement (SLA)

| Metric | Target | Penalty |
|--------|--------|---------|
| Availability | 99.95% | 5% monthly credit per 0.1% below |
| Query Response Time (p95) | <10 sec | 2% credit if >10 sec |
| Document Upload Time (p95) | <5 sec | 2% credit if >5 sec |
| Mean Time to Recovery (MTTR) | <1 hour | 5% credit if MTTR >2 hours |

### Support Tiers

| Tier | Response Time | Support Hours | Cost |
|------|---------------|---------------|------|
| **Community** | Best effort | 24/7 (async) | Free |
| **Startup** | 4 hours | 5x9 | $500/month |
| **Professional** | 1 hour | 24/7 | $2000/month |
| **Enterprise** | 15 minutes | 24/7 + dedicated | Custom |

---

## Version & Changelog

**Current Version**: 1.0.0  
**Release Date**: February 2026

### Roadmap

- [ ] **v1.1.0**: Multi-document sessions, document versioning
- [ ] **v1.2.0**: Local LLM support (Llama2 on-prem)
- [ ] **v1.3.0**: Real-time streaming responses
- [ ] **v2.0.0**: GraphQL API, multi-tenant SaaS platform

---

## Contact & Support

- **Email**: enterprise-support@example.com
- **Slack**: #ai-copilot-enterprise (private)
- **Status Page**: https://status.ai-financial-copilot.com
- **Docs**: https://api.ai-financial-copilot.com/docs

---

**Last Updated**: February 23, 2026  
**Maintained By**: Platform Engineering Team
