# ğŸ“Š AI Financial Copilot

[![CI](https://github.com/robre8/ai-financial-copilot/actions/workflows/ci.yml/badge.svg)](https://github.com/robre8/ai-financial-copilot/actions/workflows/ci.yml)

A production-ready full-stack RAG (Retrieval-Augmented Generation) system for financial document analysis. Upload PDFs, ask questions, and get AI-powered insights with a beautiful, responsive web interface.

**ğŸš€ Live Demo**: https://ai-financial-copilot-bdtd.vercel.app/  
**ğŸ“¡ API Server**: https://ai-financial-copilot-2.onrender.com

## âœ¨ Features

### Frontend (React + Vite + Tailwind)
- ğŸ¨ **Modern UI**: Professional dark mode with smooth animations
- ğŸ’¬ **Chat Interface**: Real-time conversation with AI, message history
- ğŸ“„ **PDF Upload**: Drag-and-drop PDF uploads directly in chat bar
- ğŸ” **Context Visualization**: Expandable chunks showing retrieved documents
- ğŸ¤– **Model Indicators**: Color-coded badges showing which LLM model generated each response
- ğŸ“± **Responsive Design**: Works on desktop, tablet, and mobile
- âš¡ **Real-time Loading**: Animated loading indicators during inference

### Backend (FastAPI + Python)
- **PDF Processing**: Extract and index financial documents automatically
- **Semantic Search**: Find relevant document sections using AI embeddings
- **LLM Generation**: Generate answers using Groq's lightning-fast API with 3-model fallback
- **FAISS Vector Store**: Efficient similarity search (384-dim, max 5000 vectors)
- **Groq API Integration**: Automatic fallback chain for high availability
- **REST API**: Full REST API with CORS support for cross-origin requests
- **Docker Ready**: Production-ready Docker configuration

## ğŸ› ï¸ Tech Stack

### Frontend
- **React 18** with TypeScript
- **Vite** for fast development and optimized builds
- **Tailwind CSS** for responsive, modern styling
- **Axios** for HTTP requests
- **localStorage** for chat persistence and dark mode

### Backend
- **FastAPI** with Uvicorn ASGI server
- **Python 3.11** slim Docker image
- **Groq API** for LLM inference (llama-3.1-8b, llama-3.1-70b, mixtral-8x7b)
- **Huggingface InferenceClient** for embeddings (all-MiniLM-L6-v2, 384-dim)
- **FAISS** for vector similarity search
- **PyPDF** for PDF text extraction
- **pydantic** for data validation

### Deployment
- **Backend**: Render.com (Docker, auto-deploy on git push)
- **Frontend**: Vercel (auto-deploy on git push)
- **Version Control**: GitHub monorepo

## ğŸ“‹ Requirements

### Prerequisites
- **Python 3.11+** (for local backend development)
- **Node.js 18+** and npm (for local frontend development)
- **Huggingface API Token** - [Get one here](https://huggingface.co/settings/tokens)
- **Groq API Key** - [Get one here](https://console.groq.com/keys)

### API Keys (Free Tier)
- âœ… Huggingface: Free tier sufficient (embeddings only)
- âœ… Groq: Free tier sufficient (LLM generation)

## ğŸš€ Quick Start

### Option 1: Use Live Demo (Recommended for Testing)

Simply visit: https://ai-financial-copilot-bdtd.vercel.app/

1. Upload a PDF (use the ğŸ“ button in the chat bar)
2. Wait for indexing to complete
3. Ask questions about your document
4. View answers with model info and retrieved chunks

### Option 2: Local Development

#### Backend Setup
```bash
# Clone repository
git clone https://github.com/robre8/ai-financial-copilot.git
cd ai-financial-copilot

# Create Python virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install backend dependencies
pip install -r backend/requirements.txt

# Create .env file with your tokens
cat > backend/.env << EOF
HF_TOKEN=your_huggingface_token
GROQ_API_KEY=your_groq_api_key
FRONTEND_ORIGINS=http://localhost:5173
PYTHONUNBUFFERED=1
PYTHONDONTWRITEBYTECODE=1
EOF

# Run backend server
cd backend
uvicorn app.main:app --reload
# API available at: http://localhost:8000
# Docs at: http://localhost:8000/docs
```

#### Frontend Setup
```bash
# Open new terminal in project root
cd ai-copilot-frontend

# Create environment file
cat > .env.local << EOF
VITE_API_BASE=http://localhost:8000
EOF

# Install dependencies
npm install

# Run development server
npm run dev
# Frontend available at: http://localhost:5173
```

## ğŸŒ API Endpoints

### Health Check
```bash
GET /
```
Returns: `{"message": "AI Financial Copilot API", "status": "online"}`

### Upload PDF
```bash
POST /upload-pdf
Content-Type: multipart/form-data

file: <your_pdf_file>
```
Returns: `{"message": "PDF uploaded and indexed successfully"}`

### Ask Question (Main Endpoint)
```bash
POST /ask
Content-Type: application/json

{
  "question": "What are the company's quarterly earnings?"
}
```
Returns:
```json
{
  "answer": "Based on the indexed documents...",
  "model": "llama-3.1-8b-instant",
  "chunks": ["First retrieved chunk...", "Second chunk..."],
  "context": "Full concatenated context from documents...",
  "chunk_count": 3
}
```

### Debug LLM (Testing Only)
```bash
POST /debug/llm-raw
Content-Type: application/json

{
  "prompt": "Summarize this document..."
}
```
Returns: `{"result": "...", "model": "llama-3.1-70b-versatile"}`

## ğŸ“ Project Structure

```
ai-financial-copilot/ (monorepo)
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ routes.py              # FastAPI endpoints
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py         # Groq LLM inference
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding_service.py   # Huggingface embeddings
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_store_service.py # FAISS vector DB
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf_service.py         # PDF text extraction
â”‚   â”‚   â”‚   â””â”€â”€ rag_service.py         # RAG orchestration
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py              # Configuration & env vars
â”‚   â”‚   â”‚   â””â”€â”€ logger.py              # Logging setup
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”‚   â””â”€â”€ rag_schema.py          # API request/response models
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â””â”€â”€ text_splitter.py       # Custom text chunking
â”‚   â”‚   â”œâ”€â”€ database.py                # DB utilities
â”‚   â”‚   â”œâ”€â”€ models.py                  # Data models
â”‚   â”‚   â””â”€â”€ main.py                    # FastAPI app entry point
â”‚   â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚   â””â”€â”€ Dockerfile                     # Docker configuration
â”œâ”€â”€ ai-copilot-frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â””â”€â”€ ChatInterface.tsx       # Main chat component (React)
â”‚   â”‚   â”œâ”€â”€ App.tsx                    # Root component
â”‚   â”‚   â”œâ”€â”€ main.tsx                   # Vite entry point
â”‚   â”‚   â””â”€â”€ index.css                  # Global Tailwind styles
â”‚   â”œâ”€â”€ index.html                     # HTML template
â”‚   â”œâ”€â”€ package.json                   # Node dependencies
â”‚   â”œâ”€â”€ tsconfig.json                  # TypeScript config
â”‚   â”œâ”€â”€ tailwind.config.js             # Tailwind CSS config
â”‚   â”œâ”€â”€ vite.config.ts                 # Vite build config
â”‚   â”œâ”€â”€ .env.local                     # Local dev environment
â”‚   â””â”€â”€ .env.production                # Production environment
â”œâ”€â”€ Dockerfile                         # Backend container image
â”œâ”€â”€ render.yaml                        # Render deployment config
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ”„ Architecture Flow

```
â”Œâ”€ USER INTERFACE (React Frontend) â”€â”
â”‚ - Upload PDF                      â”‚
â”‚ - Ask Questions                   â”‚
â”‚ - View Answers + Context          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP/CORS
               â†“
â”Œâ”€ FastAPI Backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POST /upload-pdf                  â”‚
â”‚   â†“ PyPDF extraction              â”‚
â”‚   â†“ Custom text splitter          â”‚
â”‚   â†“ Huggingface embeddings        â”‚
â”‚   â†“ FAISS indexing                â”‚
â”‚                                   â”‚
â”‚ POST /ask                         â”‚
â”‚   â†“ Semantic search (FAISS)       â”‚
â”‚   â†“ Retrieve top-3 chunks         â”‚
â”‚   â†“ Groq LLM with fallback        â”‚
â”‚   â†“ Return answer + metadata      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
      â†“                 â†“
   Groq API      Huggingface API
   (LLM Gen)     (Embeddings)
```

## ğŸ§  Design Decisions

**Why FAISS instead of Pinecone?**
- Chosen for zero-cost, in-memory performance within the 512MB constraint.

**Why Groq instead of OpenAI?**
- Faster inference speed and a generous free tier for prototyping.

**Why custom RAG instead of LangChain?**
- More control over memory usage and orchestration logic.

**Why 384-dim embeddings?**
- Good balance between semantic quality and memory footprint.

## âš ï¸ Known Limitations

- FAISS index is in-memory and resets on container restart.
- No authentication or rate limiting (demo purposes).
- Single-document session model.
- Scanned PDFs without selectable text are not supported (no OCR).
- No chat/session persistence across deployments (in-memory only).
- No streaming responses (answers are returned after full generation).

## ğŸ” Environment Variables

### Backend (`backend/.env`)
```bash
# Required: Huggingface API token for embeddings
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx

# Required: Groq API key for LLM
GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxx

# Required: CORS allowed origins (comma-separated)
FRONTEND_ORIGINS=http://localhost:5173,https://ai-financial-copilot-bdtd.vercel.app

# Optional: Python logging
PYTHONUNBUFFERED=1
PYTHONDONTWRITEBYTECODE=1
```

### Frontend (`.env.production`)
```bash
# Backend API URL
VITE_API_BASE=https://ai-financial-copilot-2.onrender.com
```

## ğŸš¢ Deployment

### Render (Backend)
The backend is automatically deployed when you push to GitHub's `main` branch.

**Configuration**:
- Service: https://ai-financial-copilot-2.onrender.com
- Plan: Free tier (512MB RAM, redeploys every 15 min if inactive)
- Build: Runs `pip install -r backend/requirements.txt`
- Start: Runs Docker build + Uvicorn server

**To Deploy**:
1. Push to GitHub: `git push origin main`
2. Render auto-detects changes and starts building
3. Monitor at: https://dashboard.render.com

### Vercel (Frontend)
The frontend is automatically deployed when you push to GitHub's `main` branch.

**Configuration**:
- URL: https://ai-financial-copilot-bdtd.vercel.app
- Plan: Free tier (unlimited deployments)
- Build: Runs `npm install && npm run build`
- Env vars: `.env.production` file

**To Deploy**:
1. Push to GitHub: `git push origin main`
2. Vercel auto-detects changes and starts building
3. Monitor at: https://vercel.com/dashboard

## ğŸ“š API Response Models

### QuestionResponse (POST /ask)
```json
{
  "answer": "string (AI-generated answer)",
  "model": "string (e.g. 'llama-3.1-8b-instant')",
  "chunks": ["string (retrieved document chunks)"],
  "context": "string (concatenated context used)",
  "chunk_count": "integer (number of chunks retrieved)"
}
```

### ErrorResponse
```json
{
  "detail": "string (error message)"
}
```

## ğŸ§ª Example Usage

### Using cURL

```bash
# 1. Upload PDF
curl -X POST "https://ai-financial-copilot-2.onrender.com/upload-pdf" \
  -F "file=@financial_report.pdf"

# 2. Ask question
curl -X POST "https://ai-financial-copilot-2.onrender.com/ask" \
  -H "Content-Type: application/json" \
  -d '{"question": "What was the net revenue in Q3?"}'

# 3. View response
# {
#   "answer": "Based on the financial report...",
#   "model": "llama-3.1-8b-instant",
#   "chunks": 3,
#   ...
# }
```

### Using Python

```python
import requests

API_BASE = "https://ai-financial-copilot-2.onrender.com"

# Upload PDF
with open("report.pdf", "rb") as f:
    r = requests.post(f"{API_BASE}/upload-pdf", files={"file": f})
    print(r.json())

# Ask question
response = requests.post(
    f"{API_BASE}/ask",
    json={"question": "What are the main risks?"}
)
print(response.json())
```

## âš™ï¸ Configuration

### LLM Models (Groq)
The system uses automatic fallback chain for high availability:
1. **llama-3.1-8b-instant** (fast, for simple questions)
2. **llama-3.1-70b-versatile** (balanced, for complex analysis)
3. **mixtral-8x7b-32768** (powerful, fallback)

Each model is tried in order until one succeeds.

### Embeddings (Huggingface)
- **Model**: `sentence-transformers/all-MiniLM-L6-v2`
- **Dimension**: 384-dimensional vectors
- **Speed**: ~100ms per document
- **Quality**: Good for financial documents

### Vector Store (FAISS)
- **Algorithm**: IndexFlatL2 (exact similarity search)
- **Max Vectors**: 5000 (â‰ˆ75MB memory)
- **Retrieval**: Top-3 chunks per query
- **Search Type**: Cosine similarity

### Text Chunking
- **Chunk Size**: 512 tokens per chunk
- **Overlap**: 100 tokens between chunks
- **Purpose**: Balance context window with relevance

## ğŸ› Troubleshooting

### CORS Errors
**Error**: `Cross-Origin Request Blocked`
**Solution**: 
1. Check `FRONTEND_ORIGINS` is set in backend `.env`
2. Include your Vercel URL: `https://ai-financial-copilot-bdtd.vercel.app`
3. Redeploy backend: Push to main branch

### LLM Generation Fails
**Error**: `RuntimeError: All LLM models failed`
**Solution**:
1. Verify `GROQ_API_KEY` is correct in backend `.env`
2. Check Groq API status: https://status.groq.com/
3. Check backend logs: `GET /debug/llm-raw` endpoint

### PDF Upload Issues
**Error**: `Failed to index PDF`
**Solution**:
1. Ensure PDF is not corrupted (try opening locally)
2. Check file size (max 50MB tested)
3. Verify `HF_TOKEN` is correct for embeddings

### Out of Memory
**Error**: `MemoryError` or slow responses
**Solution**:
1. Groq backend limits to 5000 vectors (auto-pruned)
2. Reduce chunk overlap in `backend/app/utils/text_splitter.py`
3. Upload fewer/smaller documents

### Frontend Blank Screen
**Error**: `VITE_API_BASE` not set or unreachable
**Solution**:
1. Check `.env.production` exists in `ai-copilot-frontend/`
2. Verify backend URL is correct and reachable
3. Check browser console (F12) for actual error

## ğŸ¤– Automation Capabilities

### System Automation

The AI Financial Copilot implements intelligent automation across all layers:

#### 1. **Automated Document Analysis**
- **Semantic Indexing**: Automatically extracts and chunks financial documents using langchain text splitters
- **Intelligent Embedding**: Documents are converted to 384-dimensional embeddings for semantic search
- **Vector Optimization**: FAISS efficiently stores and retrieves relevant document chunks in <10ms
- **No Manual Categorization**: System automatically understands document context and relevance

#### 2. **Intelligent Fallback Chain**
- **High Availability LLM**: Uses a 3-model fallback strategy
  ```
  Primary: llama-3.1-8b-instant (fastest)
    â†“ (if fails)
  Secondary: llama-3.1-70b-versatile (more capable)
    â†“ (if fails)
  Tertiary: mixtral-8x7b-32768 (alternative)
  ```
- **Graceful Degradation**: If all LLM models fail, returns raw context chunks to user
- **Automatic Retry**: Seamless model switching without user intervention
- **Error Logging**: Full audit trail for debugging and monitoring

#### 3. **Scalability & Load Distribution**

| Component | Capacity | Scaling Strategy |
|-----------|----------|------------------|
| **Concurrent Users** | 10 | Multiple backend replicas on Render |
| **Vector Store** | 5,000 vectors (384-dim) | Auto-pruning + periodic cleanup |
| **PDF Size** | Up to 50MB | Chunking + streaming processing |
| **Query Response Time** | 3-8 seconds | Model switching + caching |
| **Memory Footprint** | ~300MB | Optimized for free tier deployment |

**Horizontal Scaling**:
- Deploy multiple FastAPI instances behind a load balancer
- Use external vector DB (Weaviate, Pinecone) for unlimited scaling
- Implement Redis caching layer for frequently asked questions

#### 4. **Enterprise Microservice Integration**

The system is designed as a **production-ready microservice** that integrates seamlessly into enterprise applications:

##### a) **API-First Architecture**
```bash
# All functionality exposed via REST API
POST /ask                    # Query analysis endpoint
POST /upload-pdf            # Document ingestion
GET  /                      # Health check
POST /debug/llm-raw        # Testing endpoint
```

##### b) **Container Deployment**
- **Docker Ready**: Dockerfile included with optimized Python 3.11 slim image
- **CI/CD Pipeline**: GitHub Actions workflow for automated testing and Docker builds
- **Environment Sealed**: All dependencies pinned in `requirements.txt`
- **Kubernetes Compatible**: Can be deployed on K8s clusters with horizontal pod autoscaling

##### c) **Monitoring & Observability**
- **Structured Logging**: JSON-formatted logs for ELK stacks
- **Request Tracing**: Track requests from API â†’ embeddings â†’ LLM
- **Performance Metrics**: Response times, token usage, model selection
- **Health Endpoints**: `/` endpoint for load balancer health checks

##### d) **Integration Examples**

**Internal Tools Integration**:
```python
import requests

# Integrate into internal dashboard
response = requests.post(
    "https://your-domain.com/ask",
    json={"question": "What was Q3 revenue?"},
    timeout=10
)
answer = response.json()["answer"]
```

**Batch Processing**:
```bash
# Process multiple documents in CI/CD pipeline
for pdf in documents/*.pdf; do
    curl -F "file=@$pdf" https://your-domain.com/upload-pdf
done
```

**Custom Enterprise Extensions**:
- Add authentication layer (JWT/OAuth2)
- Implement rate limiting per user/API key
- Add analytics and usage tracking
- Custom vector DB integration (PostgreSQL pgvector)
- Multi-tenant support with document isolation

##### e) **Security Considerations**
- API key validation on all endpoints (`HF_TOKEN`, `GROQ_API_KEY`)
- CORS configured per environment (frontend origins validation)
- File upload validation (PDF-only, max 50MB)
- Memory cleanup after each request (prevents data leaks)
- No sensitive data stored in logs

### CI/CD Automation

The project includes a **comprehensive GitHub Actions workflow** (`.github/workflows/ci.yml`) that automates:

1. **Dependencies Installation**: Automated `pip install` for all Python packages
2. **Linting**: Code quality checks with flake8
3. **Unit Testing**: Full test suite coverage
   - RAG service tests (context retrieval, fallback handling)
   - API endpoint tests (upload, query, error handling)
   - 10+ test cases covering happy paths and edge cases
4. **Docker Build**: Automated image builds for both backend and frontend
5. **Coverage Reports**: Codecov integration for test coverage tracking

**Workflow Triggers**:
- âœ… Pushes to `main` and `develop` branches
- âœ… All pull requests
- âœ… Automated dependency caching (faster builds)

---

## ğŸ“Š Performance Metrics

### Backend Response Times
- **PDF Upload**: 2-5 seconds (depends on file size)
- **Embedding Generation**: ~100ms per chunk
- **FAISS Search**: <10ms for similarity search
- **LLM Generation**: 1-3 seconds (depends on model)
- **Total Query**: 3-8 seconds end-to-end

### Memory Usage
- **FastAPI Server**: ~150-200MB baseline
- **Vector Store**: ~75MB (5000 vectors)
- **Total**: ~300MB (fits in Render free tier)

### Scaling Limits
- **Max Concurrent**: 10 requests (configured)
- **Max Vectors**: 5000 (auto-pruned if exceeded)
- **Max PDF Size**: ~50MB (tested successfully)

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- [ ] Support for more document types (DOCX, TXT, CSV)
- [ ] Multiple PDFs per session
- [ ] Advanced search filters
- [ ] Chat export to PDF
- [ ] Multi-language support
- [ ] Custom LLM models
- [ ] Streaming responses

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ“ Support

- **Issues**: GitHub Issues
- **API Docs**: https://ai-financial-copilot-2.onrender.com/docs
- **Frontend**: https://ai-financial-copilot-bdtd.vercel.app/

---

**Made with â¤ï¸ using Groq, Huggingface, and FAISS** ğŸš€
